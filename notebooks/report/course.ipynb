{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aabfc47-a3af-4519-a7db-55ac9378a16f",
   "metadata": {},
   "source": [
    "# Introductie in Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79005a2-92d7-4944-821c-63a397163c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2\n",
    "\n",
    "# python core libraries\n",
    "\n",
    "# 3rd party libraries\n",
    "\n",
    "# custom libraries\n",
    "from src import visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e5b01e-4ebc-4196-ad10-31cdc36058c9",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Wat is `Machine learning`?\n",
    "\n",
    "We zijn inmiddels redelijk bekend met de term machine learning. Dit komt met name doordat Google, Apple, Tesla, etc. er geen geheim van maken en met veel trots het presenteren en in hun  devices stoppen. Maar laten we eerst even de term opdelen, en deze individueel behandelen.\n",
    "\n",
    "### Machine\n",
    "Het `Machine` gedeelte in machine learning staat voor de fysieke hardware. In je huis kan er van alles als een machine bestempeld worden je: \n",
    "- laptop\n",
    "- telefoon\n",
    "- tv\n",
    "- etc ...\n",
    "\n",
    "maar ook je:\n",
    "- auto\n",
    "- koelkast\n",
    "- wifi-router\n",
    "- etc ...\n",
    "\n",
    "Maar in het bedrijfsleven komen ook machines voor, denk hierbij aan:\n",
    "- de hoogovens in ijmuiden\n",
    "- de machines van Campina die van koemelk, gewone melk maken\n",
    "- het productie process van Nedcar\n",
    "- etc ...\n",
    "\n",
    "maar ook aan:\n",
    "- de computers (servers) die de beurs in de gaten houden\n",
    "- de computers (servers) die ons drinkwater netwerk monitoren\n",
    "- de computers (servers) die het treinverkeer regelen\n",
    "- etc ...\n",
    "\n",
    "Al deze machines / devices worden aangestuurd door een processor, en dat is nou net datgene wat we nodig hebben voor machine learning. \n",
    "\n",
    "### Learning\n",
    "Wij mensen leren in onze baby fase door naar dingen te kijken en te proeven, we leren de relaties tussen objecten (bv. gezichten) en woorden (papa, mama). Daarna gaan we naar school, daar worden ons nieuwe dingen aangeleerd doormiddel van herhalen en oefenen. En als we klaar zijn met school, dan leren we op het werk van collega's en van de fouten die we maken.\n",
    "\n",
    "Het `Learning` gedeelte in machine learning staat voor dat deze machine iets kunnen leren. Dit doen we door middel van geavanceerde wiskunde / statistiek. In de wiskunde en statistiek wordt vaak gesproken over algoritmes. **Een algoritme is een verzameling van wiskundige stappen, die in een specifieke volgorde uitgevoerd worden**. Dit wordt vaak opgeschreven in een combinatie met verschillende (griekse) symbolen. Dit ziet er vaak moeilijk uit, maar je kan het het beste vergelijken met een nieuwe taal. Ieder symbool in het algoritme heeft een naam en een betekenis alleen spreek of schrijf je (nog) niet de taal. We gaan het niet over de taal hebben, sterker nog deze proberen we te vermijden.\n",
    "\n",
    "Met het `Learning` gedeelte in machine learning, bedoelen we dus dat we een algoritme iets gaan leren op dezelfde manier zoals wij mensen het geleerd hebben, door heel veel te herhalen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8570e9b5-b217-48f1-b0af-f914bbcbce13",
   "metadata": {},
   "source": [
    "## Waarom machine learning?\n",
    "\n",
    "In een traditioneel computer programma, maken wij bijvoorbeeld iets als volgt:\n",
    "```python\n",
    "def fancy_function(input_1:int, input_2:int) -> int:\n",
    "    answer = input_1 + input_2\n",
    "    return answer\n",
    "```\n",
    "Meestal iets complexer, maar in essentie, schrijven wij een functie die gegeven bepaalde input `data` en business `rules` een output (`answer`) terug geeft. De logica en complexiteit zit hem in de business `rules`. \n",
    "\n",
    "> **Bitcoin trader example**\n",
    "> \n",
    "> Stel dat je een programma wil schrijven om de Bitcoin beurskoers te voorspellen, om precies te zijn je wilt weten of je:\n",
    "> - je aandelen moet verkopen\n",
    "> - aandelen erbij kopen\n",
    "> \n",
    "> Je begint heel simpel:\n",
    "> ```python\n",
    "> def predictor(exchange_value:int, buy_price:int) -> int:\n",
    ">     answer = 'sell' if exchange_value > buy_price else 'buy' \n",
    ">     return answer\n",
    "> ```\n",
    "> \n",
    "> hmm, als we dit aanzetten, dan zijn we na 1 ronde of alles kwijt, of we hebben verdubbeld. Als we alles hebben verkocht hebben we waarschijnlijk een kleine winst gemaakt, als we een aantal aandelen gekocht hebben dan hebben we extra geinvensteerd, en kost het alleen maar geld in plaats van dat we willen renderen. Dus je past het programma aan zodat het de koers in de gaten houdt, zodat je alleen handelt bij pieken (verkopen) en dalen (kopen), en je zet een cooldown op je aankopen. Het programma lijkt te werken en je aandelen worden mooi verhandeld zoals jij het wil, en langzaam aan neemt je rendement toe. Een vriend van je, wil het programma ook gebruiken en vraagt of je het ook kan maken voor Apple en Google beurskoerzen. Je past je code aan en nu blijkt dat het veel minder rendeert. Na wat research kom je er achter dat deze beurzen veel stabieler zijn en dus veel kleinere pieken en dalen hebben. Je past de code aan zodat ook deze beurzen een goed rendement hebben. Je merkt dat je programma het leuk doet en dat er steeds meer vraag naar is en steeds meer suggesties om beter te renderen. Je blijft logica toevoegen om de juiste keuzes te maken. \n",
    "\n",
    "> Tot het op een gegeven moment onhoudbaar wordt, je hebt zo veel logica in je programma zitten dat je het nauwelijks nog van elkaar kan onderscheiden, en er fouten in sluipen. Je bent van een klein simpel programma, geevolueerd naar een hele hoop if else statements en veel meer variabelen input, om betrouwbare antwoorden te krijgen, maar je bent nu op een punt beland dat je het niet meer kan overzien. Er zijn zoveel regels in geslopen die elkaar tegenspreken en of aanvullen, dat je nieuwe toevoegingen niet meer kan overzien.\n",
    "\n",
    "Hier komt machine learning in beeld; Bij machine learning leer je een algoritme om de business `rules` te herkennen aan de hand van de gegeven input `data` met de daarbij behorende output (`answers`).\n",
    "\n",
    "![](../../reports/figures/Machine-learning-vs-traditional-programming.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f6d621-f43c-4b12-adf4-a1db24a7b88d",
   "metadata": {},
   "source": [
    "## Hoe leren machines?\n",
    "\n",
    "Het leren van een algoritme gebeurt in 2 fases, (`training`, `inferencing`)\n",
    "\n",
    "### Training\n",
    "Voor het leren van een algoritme hebben dus de input `data` nodig en het bijbehorende `antwoord`. Het antwoord is datgene wat we willen gaan voorspellen / invullen, dit noemen we de `target`. De input data zijn de eigenschappen om tot het antwoord te komen, de input data noemen we dan ook `features`. Daarnaast hebben we veel voorbeelden, dit noemen we `observations` of `records`. De `features` worden altijd aangeduid met de hoofdletter `X`, en de target wordt altijd aangeduid met de kleine letter `y`. De dataset ziet er uit als een Excel sheet. iedere regel is een voorbeeld (`observation`) en de eigenschappen staan in de kolommen (`features (X)`) met 1 speciale kolom de `target (y)`.\n",
    "\n",
    "![](../../reports/figures/dataset.png)\n",
    "\n",
    "We gaan het algoritme nu `trainen`, dat wil zeggen we gaan het algoritme leren om de `y` te voorspellen. Dit gaat als volgt te werk:\n",
    "- **(1)**: we lopen regel voor regel door alle `observations` heen.\n",
    "  - **(1a)**: bij iedere `observation` geven we de `X` als input aan het algoritme\n",
    "  - **(1b)**: het algoritme geeft een voorspelling van y => (`y_pred`)\n",
    "  - **(1c)**: vervolgens wordt de voorspelling `y_pred` vergeleken met de echte waarde `y`\n",
    "  - **(1d)**: pas de (interne) parameters van het algoritme aan\n",
    "- **(2)**: convergeert het algoritme?\n",
    "  - **(ja)**: stop met trainen, het algoritme leert niets meer\n",
    "  - **(nee)**: ga terug naar stap 1\n",
    "  \n",
    "Convergeren betekend: \"*past het algoritme zich nog wel aan tenopzichte van de vorige ronde*\". Zoals je ziet gaat het trainen net zo lang door tot er geen (nauwelijks) wijzigingen meer zijn in de interne parameters van het algoritme. We hebben nu de (interne) parameters van het algoritme afgestemd op de dataset. Nu het algoritme getraind is noemen we het vaak een `model`. Het `model` is een representatie van de dataset geworden, het is een `model` geworden van de werkelijkheid. \n",
    "\n",
    "### Inferencing\n",
    "Bij de inferencing fase gaan we het model daardwerkelijk gebruiken (inzetten). Aan het trainen van een model alleen heb je niet veel, je wilt het uiteindelijk ook gaan inzetten. We slaan het getrainde model op. Nu kunnen we nieuwe input data `features` aanbieden aan het model om deze te laten \"*voorspellen*\" wat er gaat gebeuren. \n",
    "\n",
    "Zoals je in het onderstaande diagram ziet gebruiken we bij training de `features (X)` om de `target (y)` te \"leren\". Bij inferencing weten we alleen de features in willen we dat het `model` de `target (y)` voor ons invult.\n",
    "![](../../reports/figures/training_inferencing.png)\n",
    "\n",
    "> **F1-car example**\n",
    ">\n",
    "> Stel je dataset was de sensor-data van een formule 1 auto, die door Max Verstappen een seizoen is gebruikt. Je model is nu een digitale versie van de auto geworden, dit noemen ze vaak een `digital twin`. Je kan nu veilig vanachter je computer extreme situaties aan het model aanbieden en kijken wat het model als output geeft, je kan de situaties testen zonder iets of iemand in gevaar te brengen. \n",
    ">\n",
    "> bijvoorbeeld, je target is de maximale acceleratie snelheid van de auto, gegeven allerlei kenmerken (temperatuur van de banden, hoeveel brandstof is er aan boord, het weer, de drag coefficient etc...). Je traint het model op de historische data (ritten uit het verleden), het model kan je nu inzetten om real-time de maximale acceleratie snelheid te voorspellen. \n",
    "> - Dit kan je aan boord van de auto doen, zodat Max weet dat de auto technisch gezien mogelijk nog harder kan optrekken\n",
    "> - Dit kan je aan de zijlijn doen, om tactische keuzes te maken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08d711-71a7-4e94-b286-1526c86ae80c",
   "metadata": {},
   "source": [
    "## Welke soorten machine learning zijn er?\n",
    "\n",
    "Er zijn grofweg 3 soorten machine learning:\n",
    "\n",
    "![](../../reports/figures/soorten.jpg)\n",
    "\n",
    "### Supervised learning\n",
    "Tot nu toe hebben we het met name gehad over een algoritme, een `target (y)` kan leren gegeven de `features (X)`. Het leren van een `target` noemen we `supervised learning`. Supervised learning kan je opdelen in 2 soorten (`classification`, `regrssion`).\n",
    "\n",
    "#### Classification\n",
    "Als de `target` een textuele kolom is, dan is het waarschijnlijk een categorische kolom, en dan spreken we van `classification`. Met een categorische kolom bedoelen we een lijst van \"*beperkte*\" opties. Enkele voorbeelden zijn:\n",
    "- fraude detection: bank transactie is **`[wel / niet]`** frauduleus\n",
    "- image classification: op het plaatje staat een **`[hond / kat / albatros / zeepaard / ezel]`**\n",
    "- customer retention: klant komt **`[nooit / sporadisch / regelmatig / vaak]`** terug\n",
    "- diagnostiek: de patient heeft **`[wel / niet]`** kans op corona\n",
    "\n",
    "#### Regession\n",
    "Als de `target` een numerieke kolom is dan spreken we van `regression`. \n",
    "\n",
    "- forecasting: we voorspellen **`93,8`** % kans op regen\n",
    "- predictions: de man op de foto is **`75`** jaar oud\n",
    "- process optimization: met deze instellingen draait de fabriek op **`83`** % efficiency\n",
    "- new insights: de patient heeft **`[ < 80% / > 80%]`** kans op corona\n",
    "\n",
    "### Unsupervised learning\n",
    "Met supervised learning leren we dus een `target (y)` in te vullen. Maar in sommige situaties hebben we geen `target (y)` die we willen invullen maar willen we iets anders met de data doen. Dan komen we in het gebied van de `unsupervised learning`. Ook hierin heb je 2 smaken (`dimensionality reduction`, `clustering`)\n",
    "\n",
    "#### Dimensionality reduction\n",
    "Data kan je het beste zien als een multi-dimensionale ruimte, waarbij iedere feature een dimensie is. Stel je hebt een dataset met 1 feature (leeftijd), dan heb je een 1 dimensionale (1D) ruimte. Voor de visuele denkers, kan je dit het beste voorstellen als een punt op een lijn. Hierbij is de leeftijd de lijn, en de observatie (mens) de punt. Als je hier nu een 2e feature  (geslacht) aan toevoegd dan is je dataset 2 dimensionaal geworden. Voor de visuele denkers, er is nu een lijn (geslacht) aan toegevoegd aan het diagram en de observatie (mens) staat ook ergens op deze as. Als we nu een 3e kolom / dimensie / feature toevoegen aan de dataset (inkomen), dan hebben we een dataset met 3 dimensies (3D) die allemaal een andere as representeren. De observatie (mens) is nog steeds 1 punt ergens op de alle 3 de assen.\n",
    "\n",
    "![](../../reports/figures/dimensions.png)\n",
    "\n",
    "Maar wat als we nu meer als 3 kolommen / dimensies / features hebben? Dat valt niet (handig) meer te visualiseren, we kunnen nog wel een beetje spelen met vormen en kleuren, en afmetingen van de punt, maar dit maakt je visualisatie meestal onduidelijker. Hoe kan je hoog-dimensionsele dataset visualiseren? Je kan features weglaten die niet veel betekenen, of je kan features samenvoegen (bijvoorbeeld een nieuwe fictieve as maken van leeftijd & geslacht, dan representeerd een punt op deze as een oude-man of jonge-vrouw etc.) Maar welke features moet je weglaten of samenvoegen?? Deze stap heeft `dimensionality reduction`. \n",
    "\n",
    "Hieronder enkele voorbeelden van dimensionality reduction:\n",
    "- feature elicitation: wat zijn de \"sterke\" features in de dataset?\n",
    "- structure discovery: welke structuur zit er in de data?\n",
    "- meaningfull compression: welke features beschrijven de data het beste?\n",
    "- big data visualisation: zie beschrijving hierboven\n",
    "\n",
    "#### Clustering\n",
    "Een andere aanpak van supervised learning is `clustering`, hierbij ga je de observaties groeperen / clusteren. Naast dat het je inzicht geeft in de \"verborgen\" groepen kan je het ook inzetten voor verschillende doeleindes.\n",
    "\n",
    "Hieronder enkele voorbeelden van clustering:\n",
    "- Recommender systems: Denk aan netflix, gegeven jou playlist, welke film kunnen we het beste aanraden?\n",
    "- Targetted marketing: Welke groep mensen is het meest geschikt voor de reclame campagne?\n",
    "- Customer segmentation: Kunnen we het klantenbestand opdelen in groepen, sporadische kopers, frequente kopers, dagelijkse kopers?\n",
    "\n",
    "### Reinforcement learning\n",
    "Bij `reinforcement learning` hebben we niet zo zeer te maken met de \"traditionele\" data, maar werken we met een ander soort data. Er is een `agent` in een `environment`, de `agent` kan bepaalde `actions` uitvoeren, waarbij iedere `action` een `reward` krijgt. \n",
    "\n",
    "Dit klinkt redelijk abstract, dus ik ga het proberen wat te verduidelijken aan de hand van een voorbeeld.\n",
    "\n",
    "![](../../reports/figures/reinforcement-learning.png)\n",
    "\n",
    "Stel we hebben een zelfrijdende auto en we willen deze laten parkeren op een parkeerplaats. \n",
    "- We willen dat de auto naar het kruisje rijdt.\n",
    "- iedere tijdsstap kiezen we een actie (vooruit, achteruit, linksaf, rechtsaf) en rijdt de auto naar het betreffende vakje.\n",
    "- als we uiteindelijk bij het kruisje zijn, dan krijgen we een beloning, (deze beloning verdelen we over alle gekozen acties)\n",
    "\n",
    "De optimale route in het voorbeeld is (voorruit, linksaf, linksaf), maar hoe leert een `reinforcement learning` dit nou? De korte versie is; door het heel vaak te herhalen. En iedere ronde zetten we het kruisje ergens anders neer. De auto kiest in het begin een willekeurige actie, en zal dus heel veel gekke keuzes maken totdat hij \"toevallig\" op het kruisje beland, door de `reward` leert hij welke acties vruchtbaar waren in welke `state` en dus leert het algoritme te reageren op de `environment`. Na heel veel itteraties gaat de `agent` sneller en efficientere routes vinden.\n",
    "\n",
    "Zoals je ziet is `reinforcement learning` een reactief systeem op een omgeving, je werkt dus niet met een dataset maar meer met een omgeving. In de learning fase wordt de `environment` dan ook meestal gesimuleerd, zodat het algoritme veel fouten kan maken, en hier uiteindelijk van gaat leren. \n",
    "\n",
    "> **GO example**\n",
    ">\n",
    "> Een bekend voorbeeld van `reinforcement learning` is de toepassing ervan op het spelletje **GO**. Het spelletje GO bevat zo veel zetten (observaties), dat dit onmogelijk is vast te leggen in een \"traditionele\" dataset, laat staan er algoritmes op los te laten. Ik heb wel eens gelezen dat het spel meer zetten bevat dan het aantal zandkorrels op aarde. Met behulp van  `reinforcement learning` heeft een team van AlphaGo (Google) een agent getraind in een simulator die zo goed werd dat deze uiteindelijk in staat was om de beste (menselijke) speler op de wereld te verslaan. De Agent was zelfs in staat om nieuwe zetten te bedenken waar de beste spelers van de wereld niet opgekomen waren, en wat uiteindelijk leide tot de overwinning.\n",
    "\n",
    "Hieronder enkele voorbeelden van reinforcement learning:\n",
    "- skill acquisition: een robot die leert om een salto te maken\n",
    "- learning tasks: een robot die leert om een rubiks cube op te lossen\n",
    "- game AI: Een AI agent die een game leert\n",
    "- real-time decisions: live tactische keuzes maken bij F1 (inhalen, pitstop, afwachten, ..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8991bf7-589c-4ea5-8d7a-251922ada6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e639161-0fc0-4335-a1ea-f15757e5f604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2672b-487f-41ef-b365-97f53a6aae4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9332182c-b453-411d-9b46-4d07985aeb30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306b01ef-c4b7-4374-8778-a7d1d5e00019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad7bc4b-bec9-4f36-ad87-e7ac5900a802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a904f2-523d-4dbe-8ec5-5f8c2064c7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69efa42b-467d-4f83-b911-4a04c4b3bc4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181982ad-937f-4257-bd8e-bb2adb4e9694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e04f0d6-13ad-43d2-b8af-5b6401841f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e441a84-5e99-48a6-9629-705bb594cd1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0933ffeb-19e2-4bb0-9d7b-09d5d0c622bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "392b84b9-54eb-433d-a997-05c050bc3128",
   "metadata": {},
   "source": [
    "## Index\n",
    "\n",
    "### features & target\n",
    "\n",
    "Zoals al eerder verteld hebben supervised algoritmes `features` en een `target`. `features` zijn de kenmerken (eigenschappen) van iedere record, en de `target` is datgene dat we willen voorspellen/bepalen. In deze dataset is de `target` heel duidelijk de `spam` kolom. Maar het kan natuurlijk ook zijn dat de `target` kolom niet zo expleciet aanwezig is, maar dat je deze wel kan afleiden van andere informatie. Als we bijvoorbeeld een ongeopende email direct verwijderen `read=False` & `deleted=True`, dan zouden we hieruit kunnen afleiden dat het spam is.\n",
    "\n",
    "### Normaliseer & Standardizeer\n",
    "\n",
    "Algoritmes vermenigvuldigen en delen de input waardes met elkaar. Als je dan huizenprijzen (bv. 489.000 euro) gaat vermenigvuldigen met het aantal kamers (bv. 10). Dan heeft het aantal kamers een veel groter effect op de vermenigvuldiging dan de prijs. Het effect wil je wel behouden, maar de verschillende schalen niet.\n",
    "De meeste algoritmes werken het daarom beste als alle input in dezelfde schaal ligt. Als we van zowel de huisprijs als het aantal kamers de range schalen tussen de [0,1] dan blijft het effect behouden (aantal kamers heeft veel invloed) maar de exacte waarde veel minder. Het verschalen tussen [0,1] heet `normaliseren`.\n",
    "\n",
    "Stel je hebt in je dataset hoofdzakelijk villa's zitten van dezelfde prijsrange. Dan liggen de prijzen veel hoger (ruim in de miljoenen neem ik aan). Als je dan gaat normaliseren tussen [0,1] dan liggen de meeste waardes rond de 0.9. Dit komt omdat 1 record (bijvoorbeeld een appartement en dus een lagere prijs) nu heel veel effect heeft op de normalisering. Dan ga je vaak `standaardiseren`. Hiermee vergelijk je iedere record tenopzichte van het `gemiddelde` waardoor je een veel betere verdeling hebt. Iedere villa wordt veel zuiverder met elkaar vergeleken, en het appartement blijft een enorme uitschieter.\n",
    "\n",
    "### Split\n",
    "\n",
    "Nu willen we de algoritmes trainen op een data set, maar we willen ook weten hoe `goed` het getrainde model is wanneer deze nieuwe data krijgt. Maar als we model \"live\" moeten zetten om te bepalen hoe `goed` deze is, dan krijgen we heel veel kritiek (als het model niet `goed` is). Wat we het beste kunnen doen is de dataset `splitsen` in een `training-set` en een `test-set`.\n",
    "\n",
    "Het model laten we dan leren op alles wat in de `training-set` zit. Als dit klaar is, dan geven we vervolgens alleen de `features` van de `test-set` (`X_test`) aan het model, zodat deze gaat `voorspellen` wat de bijbehorende targets (`y_pred`) zijn. Deze `voorspellingen` (`y_pred`) ga je vervolgens vergelijken met de `echte` waardes (`y_test`), en zodoende kan je bepalen hoe goed het model werkt.\n",
    "\n",
    "### Unieke waardes\n",
    "Een kolom met alleen maar `unieke` waardes (bijvoorbeeld een ID, of naam) die hebben geen patroon. Iedere record heeft hier een unieke waarde en dus hebben deze geen voorspellende waarde. In de meeste gevallen kan je deze kolommen negeren\n",
    "\n",
    "### Categorische waardes\n",
    "Een `categorische` kolom heeft een vast aantal waardes (zoals `embarked` en `sex`, etc). Een `continue` kolom heeft doorlopende waardes (zoals `age` en `fare`), in theorie kan deze kolom oneindig veel (tussen) waardes bevatten. Nu kunnen we de feature `embarked` omzetten naar een getal, dit wordt dan een 0, 1 of 2. Maar het probleem is dat een algoritme het ziet als een `continue` waarde, en als we `embarked = [C,Q,S] = [0,1,2]` dan normaliseren, dan wordt `0` bijvoorbeeld `0.5497`. We kunnen nu niet goed terug herleiden of het nu een `0 (C)` is of een `1 (Q)`. \n",
    "\n",
    "De beste aanpak hiervoor is om `categorische` data te `One-hot-encoden`. We maken dan van 1 kolom met meerdere (vaste) waardes, meerdere kolommen met maar 1 [0,1] waardes. \n",
    "\n",
    "| categorisch | ... | hond | kat | konijn |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| hond | ... | 1 | 0 | 0 |\n",
    "| hond | ... | 1 | 0 | 0 |\n",
    "| kat | ... | 0 | 1 | 0 |\n",
    "| konijn | ... | 0 | 0 | 1 |\n",
    "| konijn | ... | 0 | 0 | 1 |\n",
    "| hond | ... | 1 | 0 | 0 |\n",
    "| kat | ... | 0 | 1 | 0 |\n",
    "\n",
    "\n",
    "### cross validate\n",
    "\n",
    "### imbalanced classes\n",
    "Er zijn verschillende technieken om deze `bias` tegen te gaan. De meest bekende zijn over-sampling en under-sampling. Bij over-sampling ga je de records aan de `minority` class toevoegen. Bij under-sampling ga je van de `majority` class, de records verwijderen.\n",
    "\n",
    "![alt text](images/Over-sample-vs-under-sample.png \"sampling\")\n",
    "\n",
    "Beide opties hebben zijn voor en nadelen. Als je bijvoorbeeld heel simplistisch records gaat dupliceren, dan zal het model sterker leren dat die kenmerken meer invloed hebben op de minority class. Je kan ook syntetische data toevoegen. dan genereer je nieuwe records aan de hand van bepaalde strategieen. Je moet dan wel goed opletten dat de nieuwe records ook echt mogelijk zijn (bijvoorbeeld een leeftijd van 400 jaar, of een gezin met 74 kinderen). \n",
    "\n",
    "Als je data gaat verwijderen, dan moet je er wel voor zorgen dat je er genoeg variantie in de dataset blijft. Als je nou net alle `outliers` weggooid dan blijft er weinig variantie over en wordt het model minder robuust voor nieuwe situaties.\n",
    "\n",
    "### Exploratory Data Analysis\n",
    "\n",
    "In de vorige opdrachten zijn we heel naar het implementeren van machine learning modellen gegaan. Dit is natuurlijk leuk en waarvoor jullie deze cursus volgen, maar dit is niet de praktijk. **In de praktijk ga je eerst jezelf bekend maken met de data.** slice en dice het in alle mogelijke vormen, maak berekeningen en bevestig hypotheses van jouw of experts om je heen. Dit heet Exploratory Data Analysis (EDA), en dat gaan we in deze opdracht samen doen.\n",
    "\n",
    "### Holdout set\n",
    "\n",
    "Ik heb het vaak zien gebeuren, dat je een model aan het ontwikkelen bent en er steeds meer logica aan je processing pipeline toevoegd om nog betere resultaten te halen. Dit is mooi en dit moet je ook vooral blijven doen. Maar de kans wordt steeds groter dat je je model evalueert op de data waarop deze getraind is. Je evaluatie is daarmee niet meer objectief. En als je dat niet in de gaten hebt en je model implementeerd in de praktijk dan kan je evaluatie behoorlijk afwijken van de praktijk. En het laatste wat je wil is een model in prodcutie brengen dat anders (slechter) presteerd dan wat je verwacht. Hiervoor gebruiken we een holdout set. Een holdout set is een subset van je originele dataset (zonder alle processing). Hier zitten dus alle eigenaardigheden in die later gaat tegen komen en aanpakt. Deze holdout maken we meteen wanneer je de data ontvangt, en raken we NIET aan.\n",
    "\n",
    "Bij de eind-evaluatie van je model dan evalueer je je model op:\n",
    "\n",
    "evaluatie dataset\tevaluatie score\topmerkingen\n",
    "trainings data\tverwacht hoge score\tDit is juist de data waarop het model getraind is, als deze score niet hoog is (in de 95%) dan gaat er ergens iets niet goed in de trainings fase\n",
    "validatie data\tverwacht hoge score\tDit is juist de data die wordt gebruikt tijdens training, om parameters aan te passen, als deze score niet nagenoeg gelijk is aan de trainings dataset gaat er iets niet goed in de trainings fase\n",
    "testing data\thopen op een hoge score\tDit is de eerste test om te beoordelen hoe goed je model is, je hoopt een zo hoog mogelijke score, het kan zijn (door verschillende redenen) dat deze net zo hoog is als de trainings score, dat is verdacht\n",
    "holdout data\thopen op een hoge score\tdeze data is negens gebruikt, alleen voor de eindevaluatie, als het model het goed doet op deze dataset dan is het waarschijnlijk dat deze in de praktijk ook net zo goed doet\n",
    "Nou kan het ook zijn dat een model het op de holdout set het goed doet, maar in de praktijk niet zo goed. Dat betekend dat je holdout set geen goeie representatie is van de praktijk. Pas je holdout set aan zodat deze ook de cases heeft waar het in de praktijk minder gaat.\n",
    "\n",
    "Bij de meeste datasets is het gebruikelijk om random (of selectieve) records te gebruiken. Bij time series is dat niet handig, want dan gaan we juist weer gaten in de serie creeren. Meestal pakken we hiervoor een x aantal observaties aan het einde van de reeks.\n",
    "\n",
    "### missing values\n",
    "Het kan voorkomen dat er in je dataset ontbrekende waardes zijn. dit wordt ook wel missing values genoemd. De oorzaak van de missing values kan diverse redenen hebben, maar geen enkel algoritme kan er mee omgaan. Je moet dus iets met deze waardes doen. Een veel-gebruikte strategie is als je voldoende (veel) records hebt je de records met missing values kan verwijderen.  Als je je niet kan veroorloven om records zo maar te verwijderen, dan moet je de lege waardes gaan invullen. Ook hier zijn verschillende technieken voor, je kan een vaste waarde overal invullen (bijvoorbeeld 0), of je kan kijken naar vergelijkbare records en daar de waardes van overnemen. Je kan zelfs een algoritme inzetten om de waardes te voorspellen, maar het belangrijkste is dat je in de gaten houdt dat deze data van invloed is op je model.\n",
    "\n",
    "### Rolling mean\n",
    "Als je naar de time series kijkt dan zien best veel pieken en dalen, dat is natuurlijk belangrijke kennis, maar kunnen we een lijn tekenen die wat minder grillig is waardoor we beter de lange termijn effecten kunnen zien.\n",
    "\n",
    "Dat kan, dit heet een rolling mean. Hierbij trekken we een `window` van een specifieke grote over de time series en berekenen het gemiddelde van alle waardes in de window. Nu schuiven we de `window` 1 record op en berkenen het gemiddelde van alle waardes in de `window`. Dit proces herhalen we tot het einde van de time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31d8fed-7d6d-4cb2-b2fe-7863fddbe2be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
